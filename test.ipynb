{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr, explode, lag\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, BooleanType, LongType, TimestampType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/homebrew/Caskroom/miniforge/base/envs/de/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/julnow/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/julnow/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b9dbafaf-efbf-49c3-b744-245299e1bd7d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1192ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b9dbafaf-efbf-49c3-b744-245299e1bd7d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/19ms)\n",
      "24/04/14 17:02:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/14 17:03:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaConsumer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"data\", StructType([\n",
    "        StructField(\"stations\", ArrayType(StructType([\n",
    "            StructField(\"is_installed\", BooleanType(), nullable=True),\n",
    "            StructField(\"is_renting\", BooleanType(), nullable=True),\n",
    "            StructField(\"is_returning\", BooleanType(), nullable=True),\n",
    "            StructField(\"last_reported\", LongType(), nullable=True),\n",
    "            StructField(\"num_bikes_available\", LongType(), nullable=True),\n",
    "            StructField(\"num_docks_available\", LongType(), nullable=True),\n",
    "            StructField(\"station_id\", StringType(), nullable=True)\n",
    "        ])))\n",
    "    ])),\n",
    "    StructField(\"last_updated\", TimestampType(), nullable=True),\n",
    "    StructField(\"ttl\", LongType(), nullable=True),\n",
    "    StructField(\"version\", StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"station_status\"\n",
    "\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse value from binay to string\n",
    "json_df = df.selectExpr(\"cast(value as string) as value\")\n",
    "\n",
    "# Apply Schema to JSON value column and expand the value\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "json_expanded_df = json_df.withColumn(\"value\", from_json(json_df[\"value\"], schema)).select(\"value.*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "exploded_df = json_expanded_df \\\n",
    "    .select(\"last_updated\", \"data\") \\\n",
    "    .withColumn(\"station_data\", explode(\"data.stations\")) \\\n",
    "    .select(\"last_updated\", \n",
    "            col(\"station_data.num_bikes_available\").alias(\"num_bikes_available\"), \n",
    "            col(\"station_data.station_id\").alias(\"station_id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, window, current_timestamp, udf\n",
    "\n",
    "windowed_df = exploded_df \\\n",
    "    .withWatermark(\"last_updated\", \"1 minute\") \\\n",
    "    .withColumn(\"window\", window(col(\"last_updated\"), \"1 minute\")) \\\n",
    "    .groupBy(\"window\", \"station_id\") \\\n",
    "    .agg({\"num_bikes_available\": \"max\"}) \\\n",
    "    .withColumnRenamed(\"max(num_bikes_available)\", \"num_bikes_available\")\n",
    "\n",
    "# Define a window to order by station_id and last_updated\n",
    "windowSpec = Window.partitionBy(\"station_id\").orderBy(\"window.start\")\n",
    "\n",
    "# Define a UDF to calculate the difference between consecutive values\n",
    "@udf(IntegerType())\n",
    "def calculate_difference(current_val, prev_val):\n",
    "    if prev_val is None:\n",
    "        return None\n",
    "    else:\n",
    "        return current_val - prev_val\n",
    "\n",
    "# Calculate the difference between consecutive updates within each window\n",
    "diff_df = windowed_df.withColumn(\"prev_num_bikes_available\", lag(\"num_bikes_available\").over(windowSpec))\n",
    "diff_df = diff_df.withColumn(\"bike_difference\", calculate_difference(col(\"num_bikes_available\"), col(\"prev_num_bikes_available\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/14 18:19:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Non-time-based windows are not supported on streaming DataFrames/Datasets;\nWindow [lag(num_bikes_available#1554L, -1, null) windowspecdefinition(station_id#1304, _w0#1560 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_num_bikes_available#1558L], [station_id#1304], [_w0#1560 ASC NULLS FIRST]\n+- Project [window#1539-T60000ms, station_id#1304, num_bikes_available#1554L, window#1539-T60000ms.start AS _w0#1560]\n   +- Project [window#1539-T60000ms, station_id#1304, max(num_bikes_available)#1549L AS num_bikes_available#1554L]\n      +- Aggregate [window#1539-T60000ms, station_id#1304], [window#1539-T60000ms, station_id#1304, max(num_bikes_available#1303L) AS max(num_bikes_available)#1549L]\n         +- Project [last_updated#1288-T60000ms, num_bikes_available#1303L, station_id#1304, window#1540-T60000ms AS window#1539-T60000ms]\n            +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - 0) % 60000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - 0) % 60000000) + 60000000) ELSE ((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - 0) % 60000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - 0) % 60000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - 0) % 60000000) + 60000000) ELSE ((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - 0) % 60000000) END) - 0) + 60000000), LongType, TimestampType))) AS window#1540-T60000ms, last_updated#1288-T60000ms, num_bikes_available#1303L, station_id#1304]\n               +- Filter isnotnull(last_updated#1288-T60000ms)\n                  +- EventTimeWatermark last_updated#1288: timestamp, 1 minutes\n                     +- Project [last_updated#1288, station_data#1299.num_bikes_available AS num_bikes_available#1303L, station_data#1299.station_id AS station_id#1304]\n                        +- Project [last_updated#1288, data#1287, station_data#1299]\n                           +- Generate explode(data#1287.stations), false, [station_data#1299]\n                              +- Project [last_updated#1288, data#1287]\n                                 +- Project [value#1285.data AS data#1287, value#1285.last_updated AS last_updated#1288, value#1285.ttl AS ttl#1289L, value#1285.version AS version#1290]\n                                    +- Project [from_json(StructField(data,StructType(StructField(stations,ArrayType(StructType(StructField(is_installed,BooleanType,true),StructField(is_renting,BooleanType,true),StructField(is_returning,BooleanType,true),StructField(last_reported,LongType,true),StructField(num_bikes_available,LongType,true),StructField(num_docks_available,LongType,true),StructField(station_id,StringType,true)),true),true)),true), StructField(last_updated,TimestampType,true), StructField(ttl,LongType,true), StructField(version,StringType,true), value#1283, Some(Europe/Warsaw)) AS value#1285]\n                                       +- Project [cast(value#1270 as string) AS value#1283]\n                                          +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4aca2f6b, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@78a95c17, [kafka.bootstrap.servers=localhost:9092, subscribe=station_status], [key#1269, value#1270, topic#1271, partition#1272, offset#1273L, timestamp#1274, timestampType#1275], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@6e2c5d73,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> station_status),None), kafka, [key#1262, value#1263, topic#1264, partition#1265, offset#1266L, timestamp#1267, timestampType#1268]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m writing_df \u001b[38;5;241m=\u001b[39m \u001b[43mdiff_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Start the streaming application to run until the following happens\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 1. Exception in the running program\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 2. Manual Interruption\u001b[39;00m\n\u001b[1;32m     10\u001b[0m writing_df\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/de/lib/python3.10/site-packages/pyspark/sql/streaming/readwriter.py:1385\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/de/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/de/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Non-time-based windows are not supported on streaming DataFrames/Datasets;\nWindow [lag(num_bikes_available#1554L, -1, null) windowspecdefinition(station_id#1304, _w0#1560 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_num_bikes_available#1558L], [station_id#1304], [_w0#1560 ASC NULLS FIRST]\n+- Project [window#1539-T60000ms, station_id#1304, num_bikes_available#1554L, window#1539-T60000ms.start AS _w0#1560]\n   +- Project [window#1539-T60000ms, station_id#1304, max(num_bikes_available)#1549L AS num_bikes_available#1554L]\n      +- Aggregate [window#1539-T60000ms, station_id#1304], [window#1539-T60000ms, station_id#1304, max(num_bikes_available#1303L) AS max(num_bikes_available)#1549L]\n         +- Project [last_updated#1288-T60000ms, num_bikes_available#1303L, station_id#1304, window#1540-T60000ms AS window#1539-T60000ms]\n            +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - 0) % 60000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - 0) % 60000000) + 60000000) ELSE ((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - 0) % 60000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - 0) % 60000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - 0) % 60000000) + 60000000) ELSE ((precisetimestampconversion(last_updated#1288-T60000ms, TimestampType, LongType) - 0) % 60000000) END) - 0) + 60000000), LongType, TimestampType))) AS window#1540-T60000ms, last_updated#1288-T60000ms, num_bikes_available#1303L, station_id#1304]\n               +- Filter isnotnull(last_updated#1288-T60000ms)\n                  +- EventTimeWatermark last_updated#1288: timestamp, 1 minutes\n                     +- Project [last_updated#1288, station_data#1299.num_bikes_available AS num_bikes_available#1303L, station_data#1299.station_id AS station_id#1304]\n                        +- Project [last_updated#1288, data#1287, station_data#1299]\n                           +- Generate explode(data#1287.stations), false, [station_data#1299]\n                              +- Project [last_updated#1288, data#1287]\n                                 +- Project [value#1285.data AS data#1287, value#1285.last_updated AS last_updated#1288, value#1285.ttl AS ttl#1289L, value#1285.version AS version#1290]\n                                    +- Project [from_json(StructField(data,StructType(StructField(stations,ArrayType(StructType(StructField(is_installed,BooleanType,true),StructField(is_renting,BooleanType,true),StructField(is_returning,BooleanType,true),StructField(last_reported,LongType,true),StructField(num_bikes_available,LongType,true),StructField(num_docks_available,LongType,true),StructField(station_id,StringType,true)),true),true)),true), StructField(last_updated,TimestampType,true), StructField(ttl,LongType,true), StructField(version,StringType,true), value#1283, Some(Europe/Warsaw)) AS value#1285]\n                                       +- Project [cast(value#1270 as string) AS value#1283]\n                                          +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4aca2f6b, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@78a95c17, [kafka.bootstrap.servers=localhost:9092, subscribe=station_status], [key#1269, value#1270, topic#1271, partition#1272, offset#1273L, timestamp#1274, timestampType#1275], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@6e2c5d73,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> station_status),None), kafka, [key#1262, value#1263, topic#1264, partition#1265, offset#1266L, timestamp#1267, timestampType#1268]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 45\n",
      "-------------------------------------------\n",
      "+------------+-------------------+----------+\n",
      "|last_updated|num_bikes_available|station_id|\n",
      "+------------+-------------------+----------+\n",
      "|  1713111597|                  2|    448565|\n",
      "|  1713111597|                  0|   2585259|\n",
      "|  1713111597|                  1|   2585263|\n",
      "|  1713111597|                  0|   2585265|\n",
      "|  1713111597|                  0|   2585267|\n",
      "|  1713111597|                  0|   2585269|\n",
      "|  1713111597|                  1|   2585271|\n",
      "|  1713111597|                  0|   2585273|\n",
      "|  1713111597|                  0|   2585275|\n",
      "|  1713111597|                  6|   2585278|\n",
      "|  1713111597|                  4|   2585280|\n",
      "|  1713111597|                  0|   2585281|\n",
      "|  1713111597|                  1|   2585283|\n",
      "|  1713111597|                  1|   2585284|\n",
      "|  1713111597|                  7|   2585285|\n",
      "|  1713111597|                  2|   2585286|\n",
      "|  1713111597|                  3|   2585288|\n",
      "|  1713111597|                  8|   2585289|\n",
      "|  1713111597|                  3|   2585291|\n",
      "|  1713111597|                  4|   2585296|\n",
      "+------------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writing_df = diff_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"checkpointLocation\",\"checkpoint_dir\") \\\n",
    "    .start()\n",
    "    \n",
    "    \n",
    "# Start the streaming application to run until the following happens\n",
    "# 1. Exception in the running program\n",
    "# 2. Manual Interruption\n",
    "writing_df.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/14 18:32:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/04/14 18:32:42 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "24/04/14 18:32:42 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "24/04/14 18:32:42 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "24/04/14 18:32:42 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "24/04/14 18:32:42 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------+-----------+-----------+\n",
      "|station_id|first_timestamp|last_timestamp|num_rentals|num_returns|\n",
      "+----------+---------------+--------------+-----------+-----------+\n",
      "|2585367   |1713108790     |1713112331    |8          |5          |\n",
      "|2585774   |1713108790     |1713112331    |1          |1          |\n",
      "|2585353   |1713108790     |1713112331    |10         |6          |\n",
      "|2585286   |1713108790     |1713112331    |4          |3          |\n",
      "|2585978   |1713108790     |1713112331    |6          |4          |\n",
      "|2586024   |1713108790     |1713112331    |7          |4          |\n",
      "|2585387   |1713108790     |1713112331    |8          |7          |\n",
      "|2585811   |1713108790     |1713112331    |6          |5          |\n",
      "|2585288   |1713108790     |1713112331    |6          |8          |\n",
      "|2585335   |1713108790     |1713112331    |4          |3          |\n",
      "|2585995   |1713108790     |1713112331    |2          |4          |\n",
      "|276784510 |1713108790     |1713112331    |1          |null       |\n",
      "|2585971   |1713108790     |1713112331    |6          |6          |\n",
      "|44583483  |1713108790     |1713112331    |8          |7          |\n",
      "|2585904   |1713108790     |1713112331    |6          |6          |\n",
      "|2585317   |1713108790     |1713112331    |11         |17         |\n",
      "|2585337   |1713108790     |1713112331    |4          |5          |\n",
      "|2585452   |1713108790     |1713112331    |9          |7          |\n",
      "|2585888   |1713108790     |1713112331    |8          |8          |\n",
      "|163208604 |1713108790     |1713112331    |null       |null       |\n",
      "+----------+---------------+--------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, explode, lag, sum as spark_sum, min as spark_min, max as spark_max, when, abs\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, BooleanType, LongType, TimestampType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import os\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaConsumer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"data\", StructType([\n",
    "        StructField(\"stations\", ArrayType(StructType([\n",
    "            StructField(\"is_installed\", BooleanType(), nullable=True),\n",
    "            StructField(\"is_renting\", BooleanType(), nullable=True),\n",
    "            StructField(\"is_returning\", BooleanType(), nullable=True),\n",
    "            StructField(\"last_reported\", LongType(), nullable=True),\n",
    "            StructField(\"num_bikes_available\", LongType(), nullable=True),\n",
    "            StructField(\"num_docks_available\", LongType(), nullable=True),\n",
    "            StructField(\"station_id\", StringType(), nullable=True)\n",
    "        ])))\n",
    "    ])),\n",
    "    StructField(\"last_updated\", LongType(), nullable=True),\n",
    "    StructField(\"ttl\", LongType(), nullable=True),\n",
    "    StructField(\"version\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Read the data in batch mode\n",
    "df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"station_status\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse value from binary to string\n",
    "json_df = df.selectExpr(\"cast(value as string) as value\")\n",
    "\n",
    "# Apply Schema to JSON value column and expand the value\n",
    "json_expanded_df = json_df.withColumn(\"value\", from_json(json_df[\"value\"], schema)).select(\"value.*\") \n",
    "\n",
    "# Explode the data\n",
    "exploded_df = json_expanded_df \\\n",
    "    .select(\"last_updated\", \"data\") \\\n",
    "    .withColumn(\"station_data\", explode(\"data.stations\")) \\\n",
    "    .select(\"last_updated\", \n",
    "            col(\"station_data.num_bikes_available\").alias(\"num_bikes_available\"), \n",
    "            col(\"station_data.station_id\").alias(\"station_id\"))\n",
    "\n",
    "# Filter data only from the last hour\n",
    "one_hour_ago = int((datetime.now() - timedelta(hours=1)).timestamp())\n",
    "filtered_df = exploded_df.filter(col(\"last_updated\") >= one_hour_ago)\n",
    "\n",
    "# Define a UDF to calculate the difference between consecutive values\n",
    "@udf(IntegerType())\n",
    "def calculate_difference(current_val, prev_val):\n",
    "    if prev_val is None:\n",
    "        return None\n",
    "    else:\n",
    "        return current_val - prev_val\n",
    "\n",
    "# Calculate the bike difference\n",
    "windowSpec = Window.partitionBy(\"station_id\").orderBy(\"last_updated\")\n",
    "diff_df = filtered_df.withColumn(\"prev_num_bikes_available\", lag(\"num_bikes_available\").over(windowSpec))\n",
    "diff_df = diff_df.withColumn(\"bike_difference\", calculate_difference(col(\"num_bikes_available\"), col(\"prev_num_bikes_available\")))\n",
    "\n",
    "# Calculate num_rentals (absolute value of negative bike differences) and num_returns\n",
    "rentals_returns_df = diff_df.groupBy(\"station_id\").agg(\n",
    "    spark_min(\"last_updated\").alias(\"first_timestamp\"),\n",
    "    spark_max(\"last_updated\").alias(\"last_timestamp\"),\n",
    "    spark_sum(when(col(\"bike_difference\") < 0, abs(col(\"bike_difference\")))).alias(\"num_rentals\"),\n",
    "    spark_sum(when(col(\"bike_difference\") > 0, col(\"bike_difference\"))).alias(\"num_returns\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "rentals_returns_df.show(truncate=False)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
